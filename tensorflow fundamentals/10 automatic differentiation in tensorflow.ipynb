{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59d7d8ba",
   "metadata": {},
   "source": [
    "#### 1. What is Automatic Differentiation?\n",
    "\n",
    "In machine learning, we need derivatives (gradients) to update weights during training.\n",
    "\n",
    "Automatic differentiation (AutoDiff) means TensorFlow calculates these gradients automatically using the computational graph, so we donâ€™t need to do manual calculus.\n",
    "\n",
    "ðŸ‘‰ TensorFlow does this with tf.GradientTape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c44eb6",
   "metadata": {},
   "source": [
    "#### 2. Basic examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2298552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x=tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y=x**2+2*x+1\n",
    "    \n",
    "dy_dx=tape.gradient(y,x)    \n",
    "print(dy_dx.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c938e18",
   "metadata": {},
   "source": [
    "#### 3) multiple variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ece60ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy_dw 3.0\n",
      "dy/db 1.0\n"
     ]
    }
   ],
   "source": [
    "w=tf.Variable(3.0)\n",
    "b=tf.Variable(1.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y=w*3+b\n",
    "    \n",
    "grads=tape.gradient(y,[w,b])\n",
    "print('dy_dw',grads[0].numpy())\n",
    "print('dy/db',grads[1].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c2cd85",
   "metadata": {},
   "source": [
    "#### 4) gradients in neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4e8a322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred:  [0.1 0.2 0.3]\n",
      "loss:  16.846666\n",
      "y_pred:  [1.8733335 3.746667  5.6200004]\n",
      "loss:  0.07487393\n",
      "y_pred:  [1.9915556 3.9831111 5.9746666]\n",
      "loss:  0.0003327744\n",
      "y_pred:  [1.9994371 3.9988742 5.998311 ]\n",
      "loss:  1.4789645e-06\n",
      "y_pred:  [1.9999626 3.9999251 5.9998875]\n",
      "loss:  6.5564905e-09\n"
     ]
    }
   ],
   "source": [
    "x=tf.constant([1,2,3],dtype=float)\n",
    "y=tf.constant([2,4,6],dtype=float)\n",
    "\n",
    "w=tf.Variable(0.1)\n",
    "learning_rate=0.1\n",
    "\n",
    "for step in range(5):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred=w*x\n",
    "        print('y_pred: ',y_pred.numpy())\n",
    "        loss=tf.reduce_mean((y-y_pred)**2)\n",
    "        print('loss: ',loss.numpy())\n",
    "        dloss_dw=tape.gradient(loss,w)\n",
    "        #print(dloss_dw.numpy())\n",
    "        w.assign_sub(learning_rate*dloss_dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6702d8d1",
   "metadata": {},
   "source": [
    "#### 5. Why is AutoDiff Important?\n",
    "\n",
    "Makes training deep networks possible without manual calculus.\n",
    "\n",
    "Works for any function (simple equations â†’ neural networks).\n",
    "\n",
    "Efficient because it only computes needed gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4340d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cleanenv)",
   "language": "python",
   "name": "cleanenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
